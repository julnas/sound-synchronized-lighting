# Music-Driven Lighting with AI

An AI-based system for **real-time music-driven lighting control**, developed as part of the university course *Advances in AI*.  
The project connects audio analysis with stage lighting (DMX/Art-Net) to generate synchronized light shows that react to music.

---

## ğŸ¯ Project Goals
- Capture audio from system loopback in real time.  
- Extract relevant features (e.g. energy, beat, spectral distribution).  
- Generate lighting cues (brightness, strobe, color, movement).  
- Compare **rule-based logic** with a **lightweight AI model**.  
- Ensure minimal latency for live synchronization.  

---

## ğŸ› ï¸ System Architecture

**Pipeline Overview:**

```
Music (Loopback) â†’ Audio Stream â†’ Feature Extraction 
â†’ [Rule Engine / AI Model] â†’ Smoothing & Cooldown â†’ DMX/Art-Net Output â†’ Lighting Fixtures
```

- **Audio Input**: Captured via system loopback (low latency).  
- **Feature Extraction**: FFT, Mel spectrogram, onset detection, energy bands.  
- **Decision Logic**:  
  - Rule-based baseline (simple mappings from features to light cues).  
  - AI model (GRU / 1D-CNN) trained on auto-labeled audio segments.  
- **Output**: Lighting frames mapped to DMX/Art-Net for stage fixtures.  
- **Stability Layer**: Exponential smoothing and cooldown timers to avoid rapid flicker and ensure professional transitions.  

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ audio_input.py       # Capture loopback audio
â”‚   â”œâ”€â”€ ringbuffer.py        # Store recent audio frames
â”‚   â”œâ”€â”€ features.py          # Compute RMS, bands, onsets, mel
â”‚   â”œâ”€â”€ decision.py          # Rule-based logic & AI model inference
â”‚   â”œâ”€â”€ smoothing.py         # EMA & cooldown handling
â”‚   â”œâ”€â”€ dmx_output.py        # Send DMX/Art-Net frames
â”‚   â””â”€â”€ run.py               # Main entry point
â”‚
â”œâ”€â”€ models/                  # Trained AI models (e.g. ONNX)
â”œâ”€â”€ data/                    # Example songs & auto-generated labels
â”œâ”€â”€ docs/                    # Reports, fixture profiles, references
â””â”€â”€ demo/                    # Demo video and screenshots
```

---

## ğŸš€ Usage

1. Clone the repository:
   ```
   git clone https://github.com/username/music-reactive-lighting.git
   cd music-reactive-lighting
   ```
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Start the system (loopback input + DMX output):
   ```
   python src/run.py --mode baseline
   ```
   or with the AI model:
   ```
   python src/run.py --mode ai --model models/tiny_lightnet.onnx
   ```

---

## ğŸ¥ Demo
A short demo video showing music-reactive lighting will be added here.  

---

## ğŸ“– Documentation
- docs/fixture_profile.md â€“ Channel layout for the lamp(s).  
- docs/report.pdf â€“ Course report including design, methodology, and results.  

---

## ğŸ§  AI Component
- **Training Data**: Auto-labeled using rule-based heuristics.  
- **Model**: Lightweight GRU/1D-CNN predicting pattern + parameters.  
- **Evaluation**: Comparison of latency, smoothness, and musical alignment against baseline rules.  

---

## âš ï¸ Limitations
- Prototype supports only a single fixture profile.  
- AI model trained on a small dataset (demonstration purpose).  
- Real-time performance depends on audio backend and DMX interface.  

---

## ğŸ“š References
- Open Lighting Architecture (OLA)  
- librosa: Python library for audio analysis  
- Research on music-to-light AI mapping (see docs/references.md)  

---

## ğŸ‘¤ Author
Developed by *[Your Name]* for the course *Advances in AI*.  
